#!/usr/bin/env python3
"""æ‰¹é‡å¯¼å…¥æ‰€æœ‰å¹´çº§çš„è‹±è¯­å•è¯åˆ°æ•°æ®åº“"""

import os
import sys
from pathlib import Path
from docx import Document
import psycopg2
from psycopg2 import sql
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ•°æ®åº“è¿æ¥
DB_URL = os.getenv('DATABASE_URL')

def parse_word_document(docx_path):
    """
    è§£æå•è¯æ–‡æ¡£ï¼Œæå–å•è¯ã€éŸ³æ ‡ã€é‡Šä¹‰
    æ–‡æ¡£æ ¼å¼ç¤ºä¾‹ï¼š
    åºå·.________[éŸ³æ ‡] è¯æ€§.ä¸­æ–‡é‡Šä¹‰
    å¦‚ï¼š3.________[ËˆkÉ”:nÉ™(r)] n.è§’
    """
    doc = Document(docx_path)
    words = []

    for paragraph in doc.paragraphs:
        text = paragraph.text.strip()
        if not text or text.startswith('#') or text.startswith('â˜…'):
            continue

        # è·³è¿‡æ ‡é¢˜è¡Œ
        if 'å•è¯è¡¨' in text or 'å¹´çº§' in text or 'ä¸Šå†Œ' in text or 'ä¸‹å†Œ' in text:
            continue

        # è§£ææ ¼å¼ï¼šåºå·.________[éŸ³æ ‡] è¯æ€§.ä¸­æ–‡é‡Šä¹‰
        # æˆ–ï¼šåºå·.________[éŸ³æ ‡] ä¸­æ–‡é‡Šä¹‰

        word = ""
        phonetic = ""
        meaning = ""
        part_of_speech = ""

        # å…ˆæå–åºå·
        if '.' in text:
            parts = text.split('.', 1)
            if parts[0].strip().isdigit():
                text = parts[1].strip()

        # è·³è¿‡ç©ºç™½å•è¯
        if text.startswith('________'):
            continue

        # æå–éŸ³æ ‡ [xxx]
        if '[' in text and ']' in text:
            start = text.find('[')
            end = text.find(']')
            word = text[:start].strip()
            phonetic = text[start:end+1].strip()
            text = text[end+1:].strip()

            # æå–è¯æ€§å’Œé‡Šä¹‰
            # æ ¼å¼å¯èƒ½æ˜¯ï¼šn.è§’ æˆ– n. è§’
            # æå–ç¬¬ä¸€ä¸ªè¯æ€§æ ‡è®°
            pos_patterns = ['n.', 'v.', 'adj.', 'adv.', 'prep.', 'pron.', 'conj.', 'interj.', 'art.']
            for pos in pos_patterns:
                if text.startswith(pos):
                    text = text[len(pos):].strip()
                    part_of_speech = pos.rstrip('.')
                    break
                elif text.startswith(pos[:-1] + ' '):
                    text = text[len(pos[:-1] + ' '):].strip()
                    part_of_speech = pos.rstrip('.')
                    break

            meaning = text

        # å¤„ç†æ²¡æœ‰éŸ³æ ‡çš„æƒ…å†µï¼ˆåªæœ‰ä¸­æ–‡ï¼‰
        elif not any(c.isalpha() for c in text.split()[0]):
            # å¯èƒ½åªæœ‰ä¸­æ–‡é‡Šä¹‰ï¼Œæ²¡æœ‰è‹±æ–‡å•è¯
            continue

        else:
            # å°è¯•å…¶ä»–æ ¼å¼
            parts = text.split(None, 2)
            if len(parts) >= 2:
                word = parts[0].strip()
                if '[' in parts[1]:
                    phonetic = parts[1].strip()
                    if len(parts) > 2:
                        meaning = parts[2].strip()
                else:
                    meaning = ' '.join(parts[1:]).strip()

        # æ¸…ç†å’ŒéªŒè¯
        word = word.replace('________', '').strip()
        if not word or len(word) < 2:
            continue

        # è¿‡æ»¤éå•è¯å­—ç¬¦ï¼ˆå…è®¸è¿å­—ç¬¦å’Œæ’‡å·ï¼‰
        cleaned_word = word.replace('-', '').replace("'", "")
        if not cleaned_word.isalpha():
            continue

        # å¿…é¡»æœ‰é‡Šä¹‰
        if not meaning or meaning.strip() == '':
            continue

        words.append({
            'word': word,
            'phonetic': phonetic,
            'meaning': meaning,
            'part_of_speech': part_of_speech,
            'source': os.path.basename(docx_path)
        })

    return words

def insert_words_to_db(words):
    """æ’å…¥å•è¯åˆ°æ•°æ®åº“"""
    conn = psycopg2.connect(DB_URL)
    cursor = conn.cursor()

    inserted_count = 0
    skipped_count = 0
    error_count = 0

    for word_data in words:
        try:
            cursor.execute("""
                INSERT INTO words (word, phonetic, meaning, difficulty)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (word) DO NOTHING
            """, (
                word_data['word'],
                word_data.get('phonetic') or None,
                word_data.get('meaning') or None,
                2,  # é»˜è®¤éš¾åº¦
            ))

            if cursor.rowcount > 0:
                inserted_count += 1
            else:
                skipped_count += 1

        except Exception as e:
            error_count += 1
            print(f"âŒ æ’å…¥å¤±è´¥ {word_data['word']}: {e}")

    conn.commit()
    cursor.close()
    conn.close()

    return inserted_count, skipped_count, error_count

def main():
    print("=" * 60)
    print("æ‰¹é‡å¯¼å…¥è‹±è¯­å•è¯åˆ°æ•°æ®åº“")
    print("=" * 60)

    # æŸ¥æ‰¾æ‰€æœ‰ .docx æ–‡ä»¶
    assets_dir = Path('/workspace/projects/assets')
    docx_files = list(assets_dir.glob('*.docx'))

    if not docx_files:
        print("âŒ æœªæ‰¾åˆ° .docx æ–‡ä»¶")
        sys.exit(1)

    print(f"\næ‰¾åˆ° {len(docx_files)} ä¸ªæ–‡æ¡£ï¼š")
    for f in docx_files:
        print(f"  - {f.name}")

    # è§£ææ‰€æœ‰æ–‡æ¡£
    all_words = []
    for docx_file in docx_files:
        print(f"\nğŸ“– æ­£åœ¨è§£æ: {docx_file.name}")
        words = parse_word_document(docx_file)
        print(f"  âœ… æå–åˆ° {len(words)} ä¸ªå•è¯")
        all_words.extend(words)

    print(f"\nğŸ“Š æ€»è®¡æå– {len(all_words)} ä¸ªå•è¯")

    # å»é‡
    unique_words = {}
    for w in all_words:
        if w['word'] not in unique_words:
            unique_words[w['word']] = w

    unique_word_list = list(unique_words.values())
    print(f"ğŸ“Š å»é‡å: {len(unique_word_list)} ä¸ªå•è¯")

    # å¯¼å…¥æ•°æ®åº“
    print("\nğŸ“¥ å¼€å§‹å¯¼å…¥æ•°æ®åº“...")
    inserted, skipped, errors = insert_words_to_db(unique_word_list)

    print(f"\n" + "=" * 60)
    print("å¯¼å…¥å®Œæˆï¼")
    print(f"âœ… æˆåŠŸæ’å…¥: {inserted} ä¸ª")
    print(f"â­ï¸  è·³è¿‡é‡å¤: {skipped} ä¸ª")
    print(f"âŒ å¤±è´¥: {errors} ä¸ª")
    print(f"ğŸ“Š æ•°æ®åº“æ€»å•è¯æ•°: {inserted + skipped} ä¸ª")
    print("=" * 60)

if __name__ == '__main__':
    main()
